{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7669478,"sourceType":"datasetVersion","datasetId":4473093},{"sourceId":8672244,"sourceType":"datasetVersion","datasetId":5197562}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Layer, Input,Dropout\nfrom keras.models import Model,Sequential\nfrom keras.layers import LayerNormalization","metadata":{"id":"7JfYqqW4rThJ","execution":{"iopub.status.busy":"2024-06-16T14:53:02.869135Z","iopub.execute_input":"2024-06-16T14:53:02.869414Z","iopub.status.idle":"2024-06-16T14:53:15.592443Z","shell.execute_reply.started":"2024-06-16T14:53:02.869390Z","shell.execute_reply":"2024-06-16T14:53:15.591462Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-16 14:53:04.650052: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-16 14:53:04.650141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-16 14:53:04.780168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"pos encoding","metadata":{"id":"VtfEh3lOrsuT"}},{"cell_type":"code","source":"def pos_encoding(max,input):\n  d= input.shape[1]\n  c=int(d/2)\n  for pos in range(0,max):\n    for i in range(0, c):\n      pos[2*i] = np.sin(pos/(10000**( (2*i)/d)))\n      pos[2*i + 1] = np.cos(pos/(10000**( (2*i)/d)))\n\n  return pos_enc","metadata":{"id":"GB09HeaNrqZx","execution":{"iopub.status.busy":"2024-06-15T18:19:25.420798Z","iopub.execute_input":"2024-06-15T18:19:25.421336Z","iopub.status.idle":"2024-06-15T18:19:25.427329Z","shell.execute_reply.started":"2024-06-15T18:19:25.421310Z","shell.execute_reply":"2024-06-15T18:19:25.426396Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"psQqYmWMoGgY","outputId":"043e1ead-7487-4196-df8c-441b5335ffeb","execution":{"iopub.status.busy":"2024-06-15T18:19:25.428512Z","iopub.execute_input":"2024-06-15T18:19:25.428880Z","iopub.status.idle":"2024-06-15T18:19:26.233395Z","shell.execute_reply.started":"2024-06-15T18:19:25.428829Z","shell.execute_reply":"2024-06-15T18:19:26.228602Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pos_enc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m10\u001b[39m));\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m   pos_enc[i] \u001b[38;5;241m=\u001b[39m \u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_enc)\n","Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mpos_encoding\u001b[0;34m(max, input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_encoding\u001b[39m(\u001b[38;5;28mmax\u001b[39m,\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m   d\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m   c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(d\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mmax\u001b[39m):\n","\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'int' object has no attribute 'shape'","output_type":"error"}]},{"cell_type":"markdown","source":"Glove vector","metadata":{"id":"tGrllADkrvNH"}},{"cell_type":"code","source":"pip install nltk","metadata":{"id":"S0CCrSEorxoN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1209d6eb-d11c-4324-cd2b-0c2d78e497c8","execution":{"iopub.status.busy":"2024-06-16T14:53:15.593721Z","iopub.execute_input":"2024-06-16T14:53:15.594416Z","iopub.status.idle":"2024-06-16T14:53:29.002676Z","shell.execute_reply.started":"2024-06-16T14:53:15.594379Z","shell.execute_reply":"2024-06-16T14:53:29.001483Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nimport string\n\n# Download necessary datasets\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkjqqageq20-","outputId":"9bf24516-573a-4fed-fb9b-b09e1f677f29","execution":{"iopub.status.busy":"2024-06-16T14:53:29.005227Z","iopub.execute_input":"2024-06-16T14:53:29.005530Z","iopub.status.idle":"2024-06-16T14:53:29.139160Z","shell.execute_reply.started":"2024-06-16T14:53:29.005502Z","shell.execute_reply":"2024-06-16T14:53:29.138073Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!wget --no-check-certificate https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZwg2Nezq5ot","outputId":"58a38dd9-2883-47a8-f93b-88be9ba6d0a9","execution":{"iopub.status.busy":"2024-06-15T18:19:55.647906Z","iopub.execute_input":"2024-06-15T18:19:55.648268Z","iopub.status.idle":"2024-06-15T18:22:35.553235Z","shell.execute_reply.started":"2024-06-15T18:19:55.648235Z","shell.execute_reply":"2024-06-15T18:22:35.551861Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-06-15 18:19:56--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: 'glove.6B.zip'\n\nglove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n\n2024-06-15 18:22:35 (5.18 MB/s) - 'glove.6B.zip' saved [862182613/862182613]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip glove.6B.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mf1xO3SYrDAu","outputId":"63b8f7ee-f2a4-4052-9d62-ae22565dbffc","execution":{"iopub.status.busy":"2024-06-15T18:22:35.557432Z","iopub.execute_input":"2024-06-15T18:22:35.558321Z","iopub.status.idle":"2024-06-15T18:22:56.279788Z","shell.execute_reply.started":"2024-06-15T18:22:35.558268Z","shell.execute_reply":"2024-06-15T18:22:56.278668Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Archive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np","metadata":{"id":"1V9dQczwrFeh","execution":{"iopub.status.busy":"2024-06-15T18:22:56.281226Z","iopub.execute_input":"2024-06-15T18:22:56.281543Z","iopub.status.idle":"2024-06-15T18:22:56.286216Z","shell.execute_reply.started":"2024-06-15T18:22:56.281515Z","shell.execute_reply":"2024-06-15T18:22:56.285284Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"glove_dir = './'\n\nembeddings_index = {} #initialize dictionary\nf = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4amojhPrHD1","outputId":"449fcc2a-a3f9-4e09-fe7c-603217cb54eb","execution":{"iopub.status.busy":"2024-06-15T18:22:56.287433Z","iopub.execute_input":"2024-06-15T18:22:56.287798Z","iopub.status.idle":"2024-06-15T18:23:28.043822Z","shell.execute_reply.started":"2024-06-15T18:22:56.287744Z","shell.execute_reply":"2024-06-15T18:23:28.042891Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"IrgRfpOex3g7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"mFRSuccnyb2i","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"7a647015-e699-4931-9c76-43c010a2cc00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"L95-H0lxyp-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"WOtEMMtIy00H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7Nd8NDVbrVFb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3fb629d1-6365-4d09-9e33-2b55fe5da85b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import zipfile\n# import os\n\n# # Path to the zip file\n# zip_file_path = \"/content/sobhins-classic.zip\"  # Update with your zip file path\n\n# # Directory to extract the files\n# extract_dir = \"/content/sample\"  # Update with the directory where you want to extract the files\n\n# # Check if the zip file exists\n# if os.path.exists(zip_file_path):\n#     # Create the extraction directory if it doesn't exist\n#     os.makedirs(extract_dir, exist_ok=True)\n\n#     # Extract the zip file\n#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n#         zip_ref.extractall(extract_dir)\n\n#     print(\"Extraction completed successfully.\")\n# else:\n#     print(\"Zip file not found.\")\n","metadata":{"id":"deFi2YpTzBto","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ef5201e-e9e5-4b11-ac1e-d8ad5d7be8f1","execution":{"iopub.status.busy":"2024-06-15T18:23:28.045098Z","iopub.execute_input":"2024-06-15T18:23:28.045473Z","iopub.status.idle":"2024-06-15T18:23:28.050409Z","shell.execute_reply.started":"2024-06-15T18:23:28.045428Z","shell.execute_reply":"2024-06-15T18:23:28.049418Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/sobhins-classic/output2.csv')","metadata":{"id":"7e5WLknGzYBf","execution":{"iopub.status.busy":"2024-06-16T13:54:00.270335Z","iopub.execute_input":"2024-06-16T13:54:00.270704Z","iopub.status.idle":"2024-06-16T13:54:01.127902Z","shell.execute_reply.started":"2024-06-16T13:54:00.270674Z","shell.execute_reply":"2024-06-16T13:54:01.126985Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df1 = df.dropna()\ndf2 = df1[['desc','genre']].copy()\ndf3 = df2.loc[:,:]\ndfm = df3[df3['genre'].isin([' drama ',' documentary ',' comedy '])]","metadata":{"id":"PY1RG1GD-k-x","execution":{"iopub.status.busy":"2024-06-16T13:54:01.129078Z","iopub.execute_input":"2024-06-16T13:54:01.129333Z","iopub.status.idle":"2024-06-16T13:54:01.167575Z","shell.execute_reply.started":"2024-06-16T13:54:01.129312Z","shell.execute_reply":"2024-06-16T13:54:01.166371Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x_train = dfm['desc']\ny_train = dfm['genre']","metadata":{"id":"ejD_44-x-_f9","execution":{"iopub.status.busy":"2024-06-16T13:54:01.168668Z","iopub.execute_input":"2024-06-16T13:54:01.168990Z","iopub.status.idle":"2024-06-16T13:54:01.174408Z","shell.execute_reply.started":"2024-06-16T13:54:01.168963Z","shell.execute_reply":"2024-06-16T13:54:01.173194Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet","metadata":{"id":"XnqR9ULFD0IJ","execution":{"iopub.status.busy":"2024-06-16T14:53:29.140216Z","iopub.execute_input":"2024-06-16T14:53:29.140464Z","iopub.status.idle":"2024-06-16T14:53:29.172546Z","shell.execute_reply.started":"2024-06-16T14:53:29.140443Z","shell.execute_reply":"2024-06-16T14:53:29.171571Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /kaggle/working/corpora/wordnet.zip\n","output_type":"stream"},{"name":"stderr","text":"replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"o7Mg8sKs_CwT","execution":{"iopub.status.busy":"2024-06-16T13:54:19.217413Z","iopub.execute_input":"2024-06-16T13:54:19.217727Z","iopub.status.idle":"2024-06-16T13:55:07.622343Z","shell.execute_reply.started":"2024-06-16T13:54:19.217704Z","shell.execute_reply":"2024-06-16T13:55:07.621323Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"word_index.size()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:55:46.078061Z","iopub.execute_input":"2024-06-16T13:55:46.078392Z","iopub.status.idle":"2024-06-16T13:55:46.109088Z","shell.execute_reply.started":"2024-06-16T13:55:46.078369Z","shell.execute_reply":"2024-06-16T13:55:46.107341Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mword_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"],"ename":"AttributeError","evalue":"'dict' object has no attribute 'size'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:39:05.228108Z","iopub.execute_input":"2024-06-16T14:39:05.228487Z","iopub.status.idle":"2024-06-16T14:39:05.371823Z","shell.execute_reply.started":"2024-06-16T14:39:05.228463Z","shell.execute_reply":"2024-06-16T14:39:05.370631Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"                                                  de  \\\n0  An der B 211 befindet sich in Loyermoor der so...   \n1  Ich begrüße die Erklärung des Herrn Kommissar ...   \n2  Das ist das Gegenteil von dem, was getan werde...   \n3                                                  .   \n4  The Ethnographical museum in Varna is in a hou...   \n\n                                                  en  \n0  Here the largest town of the district is locat...  \n1  I should like, in passing, to pay tribute to t...  \n2  That is the opposite of what should be done an...  \n3                                                  .  \n4  It was designed by the Viennese architect Rupp...  \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:55:41.935601Z","iopub.execute_input":"2024-06-16T14:55:41.936266Z","iopub.status.idle":"2024-06-16T14:55:41.940275Z","shell.execute_reply.started":"2024-06-16T14:55:41.936234Z","shell.execute_reply":"2024-06-16T14:55:41.939397Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_clean = pd.read_csv('/kaggle/input/wmt-2014-english-german/wmt14_translate_de-en_train.csv',nrows=500000,\n                 lineterminator='\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:58:53.846230Z","iopub.execute_input":"2024-06-16T16:58:53.846953Z","iopub.status.idle":"2024-06-16T16:58:56.057151Z","shell.execute_reply.started":"2024-06-16T16:58:53.846919Z","shell.execute_reply":"2024-06-16T16:58:56.056078Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"df_ger = df_clean['de']\ndf_eng = df_clean['en']","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:58:58.098392Z","iopub.execute_input":"2024-06-16T16:58:58.099306Z","iopub.status.idle":"2024-06-16T16:58:58.104026Z","shell.execute_reply.started":"2024-06-16T16:58:58.099270Z","shell.execute_reply":"2024-06-16T16:58:58.103011Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = df_eng","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:59:01.639737Z","iopub.execute_input":"2024-06-16T16:59:01.640498Z","iopub.status.idle":"2024-06-16T16:59:01.644951Z","shell.execute_reply.started":"2024-06-16T16:59:01.640449Z","shell.execute_reply":"2024-06-16T16:59:01.643908Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.datasets import imdb\n\n\n# Concatenate train and test data for preprocessing\n\n\n# Function to preprocess text\ndef preprocess_text(text):\n    \"\"\"\n    Preprocesses the input text by performing tokenization, converting to lowercase,\n    removing punctuation, removing stopwords, and lemmatization.\n\n    Args:\n    text (str): The input text to be processed.\n\n    Returns:\n    List[str]: The list of preprocessed tokens.\n    \"\"\"\n    # Tokenization\n    tokens = word_tokenize(text)\n\n    # Convert to Lowercase\n    tokens = [word.lower() for word in tokens]\n\n    # Remove Punctuation\n    tokens = [word for word in tokens if word.isalpha()]\n\n    #Remove Stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n\n#     # Lemmatization\n#     lemmatizer = WordNetLemmatizer()\n#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n    return tokens\n\n# Preprocess all data\npreprocessed_data = [preprocess_text(review) for review in x_train]\n\n\n# Tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([' '.join(review) for review in preprocessed_data])\n\n\nsequences = tokenizer.texts_to_sequences([' '.join(review) for review in preprocessed_data])\nsequences = [arr + [-2] for arr in sequences]\n\nword_index = tokenizer.word_index\n\n# Pad sequences\nmax_length = 300\npadded_dataE = pad_sequences(sequences, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:00:46.849643Z","iopub.execute_input":"2024-06-16T17:00:46.850303Z","iopub.status.idle":"2024-06-16T17:05:11.908597Z","shell.execute_reply.started":"2024-06-16T17:00:46.850272Z","shell.execute_reply":"2024-06-16T17:05:11.907645Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"last_elements[2]","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:34:52.215634Z","iopub.execute_input":"2024-06-16T16:34:52.216448Z","iopub.status.idle":"2024-06-16T16:34:52.222220Z","shell.execute_reply.started":"2024-06-16T16:34:52.216409Z","shell.execute_reply":"2024-06-16T16:34:52.221259Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"991"},"metadata":{}}]},{"cell_type":"code","source":"len(word_index)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:05:32.145185Z","iopub.execute_input":"2024-06-16T17:05:32.145558Z","iopub.status.idle":"2024-06-16T17:05:32.151813Z","shell.execute_reply.started":"2024-06-16T17:05:32.145529Z","shell.execute_reply":"2024-06-16T17:05:32.150835Z"},"trusted":true},"execution_count":178,"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"178057"},"metadata":{}}]},{"cell_type":"code","source":"word_index['<StarT>']=-1\nword_index['<EnD>']=-2","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:05:44.660752Z","iopub.execute_input":"2024-06-16T17:05:44.661499Z","iopub.status.idle":"2024-06-16T17:05:44.665867Z","shell.execute_reply.started":"2024-06-16T17:05:44.661467Z","shell.execute_reply":"2024-06-16T17:05:44.664802Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"X_train=np.array(padded_dataE)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:05:41.898359Z","iopub.execute_input":"2024-06-16T17:05:41.898870Z","iopub.status.idle":"2024-06-16T17:05:42.117704Z","shell.execute_reply.started":"2024-06-16T17:05:41.898832Z","shell.execute_reply":"2024-06-16T17:05:42.116775Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"x= np.zeros((500000,301))\nfor i in range(500000):\n    x[i] = np.insert(X_train[i],0,-1)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:06:01.700113Z","iopub.execute_input":"2024-06-16T17:06:01.700755Z","iopub.status.idle":"2024-06-16T17:06:12.849328Z","shell.execute_reply.started":"2024-06-16T17:06:01.700722Z","shell.execute_reply":"2024-06-16T17:06:12.848413Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"x= x[:,:-1]","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:06:12.851215Z","iopub.execute_input":"2024-06-16T17:06:12.851606Z","iopub.status.idle":"2024-06-16T17:06:12.856357Z","shell.execute_reply.started":"2024-06-16T17:06:12.851555Z","shell.execute_reply":"2024-06-16T17:06:12.855442Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"X_trainD=x","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:06:15.399331Z","iopub.execute_input":"2024-06-16T17:06:15.399755Z","iopub.status.idle":"2024-06-16T17:06:15.404661Z","shell.execute_reply.started":"2024-06-16T17:06:15.399723Z","shell.execute_reply":"2024-06-16T17:06:15.403629Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:51:28.956788Z","iopub.execute_input":"2024-06-16T15:51:28.957401Z","iopub.status.idle":"2024-06-16T15:51:28.992886Z","shell.execute_reply.started":"2024-06-16T15:51:28.957371Z","shell.execute_reply":"2024-06-16T15:51:28.991618Z"},"trusted":true},"execution_count":66,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_trainD \u001b[38;5;241m=\u001b[39m \u001b[43mX_trainD\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"],"ename":"IndexError","evalue":"too many indices for array: array is 1-dimensional, but 2 were indexed","output_type":"error"}]},{"cell_type":"code","source":"X_train = np.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_trainG = df_clean['de']","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:21:07.655518Z","iopub.execute_input":"2024-06-16T15:21:07.655971Z","iopub.status.idle":"2024-06-16T15:21:07.720010Z","shell.execute_reply.started":"2024-06-16T15:21:07.655942Z","shell.execute_reply":"2024-06-16T15:21:07.718896Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"x_trainG[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:23:30.172409Z","iopub.execute_input":"2024-06-16T15:23:30.172878Z","iopub.status.idle":"2024-06-16T15:23:30.179560Z","shell.execute_reply.started":"2024-06-16T15:23:30.172842Z","shell.execute_reply":"2024-06-16T15:23:30.178483Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'An der B 211 befindet sich in Loyermoor der so genannte „Geest-Abbruch“, der eine Höhendifferenz von gut 30 Meter überbrückt.'"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:22:34.549295Z","iopub.execute_input":"2024-06-16T15:22:34.549931Z","iopub.status.idle":"2024-06-16T15:22:34.556102Z","shell.execute_reply.started":"2024-06-16T15:22:34.549900Z","shell.execute_reply":"2024-06-16T15:22:34.555229Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\ndef preprocess_german_text(text):\n    # Tokenization\n    tokens = word_tokenize(text, language='german')\n    \n    # Lowercasing\n    tokens = [token.lower() for token in tokens]\n    \n    # Remove punctuation and non-alphanumeric tokens\n    tokens = [token for token in tokens if token.isalpha()]\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('german'))\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    # Lemmatization (optional)\n    # lemmatizer = WordNetLemmatizer()\n    # tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    \n    # Join tokens back into a sentence\n    preprocessed_text = ' '.join(tokens)\n    \n    return preprocessed_text\n\npreprocessed_dataG = [preprocess_german_text(review) for review in x_trainG]\n\n\n# Tokenizer\ntokenizerG = Tokenizer()\ntokenizerG.fit_on_texts([' '.join(review) for review in preprocessed_dataG])\n\n\nsequencesG = tokenizerG.texts_to_sequences([' '.join(review) for review in preprocessed_dataG])\n\n\nword_indexG = tokenizerG.word_index\n\n# Pad sequences\nmax_length = 300\npadded_data = pad_sequences(sequencesG, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:27:34.545078Z","iopub.execute_input":"2024-06-16T15:27:34.545704Z","iopub.status.idle":"2024-06-16T15:27:37.933731Z","shell.execute_reply.started":"2024-06-16T15:27:34.545673Z","shell.execute_reply":"2024-06-16T15:27:37.932859Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install german","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:30:03.068724Z","iopub.execute_input":"2024-06-16T15:30:03.069295Z","iopub.status.idle":"2024-06-16T15:30:19.932729Z","shell.execute_reply.started":"2024-06-16T15:30:03.069265Z","shell.execute_reply":"2024-06-16T15:30:19.931597Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting german\n  Downloading german-0.1.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting german-lemmatizer (from german)\n  Downloading german_lemmatizer-0.1.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting clean-text (from german)\n  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from german) (1.4.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from german) (4.66.4)\nCollecting unidecode (from german)\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nCollecting emoji<2.0.0,>=1.0.0 (from clean-text->german)\n  Downloading emoji-1.7.0.tar.gz (175 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy<7.0,>=6.0 (from clean-text->german)\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from german-lemmatizer->german) (7.0.0)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy<7.0,>=6.0->clean-text->german) (0.2.13)\nRequirement already satisfied: packaging>=14.0 in /opt/conda/lib/python3.10/site-packages (from docker->german-lemmatizer->german) (21.3)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from docker->german-lemmatizer->german) (2.32.3)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker->german-lemmatizer->german) (1.26.18)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=14.0->docker->german-lemmatizer->german) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker->german-lemmatizer->german) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker->german-lemmatizer->german) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker->german-lemmatizer->german) (2024.2.2)\nDownloading german-0.1.0-py3-none-any.whl (3.3 kB)\nDownloading clean_text-0.6.0-py3-none-any.whl (11 kB)\nDownloading german_lemmatizer-0.1.1-py3-none-any.whl (4.5 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=b0cb4b73b8f80e4e3e736c8422a72d1f8ee3e6faff8d1302dec0632f04031183\n  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\nSuccessfully built emoji\nInstalling collected packages: emoji, unidecode, ftfy, clean-text, german-lemmatizer, german\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.12.1\n    Uninstalling emoji-2.12.1:\n      Successfully uninstalled emoji-2.12.1\nSuccessfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.2.0 german-0.1.0 german-lemmatizer-0.1.1 unidecode-1.3.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from german import preprocess\n\ndef preprocess_german_text(text):\n    \n    preprocessed_text = preprocess(text, remove_stop=True)\n\n    return preprocessed_text\n\n# Preprocess all data\npreprocessed_dataG = [preprocess_german_text(review) for review in x_trainG]\n\n\n# Tokenizer\ntokenizerG = Tokenizer()\ntokenizerG.fit_on_texts([' '.join(review) for review in preprocessed_dataG])\n\n\nsequencesG = tokenizerG.texts_to_sequences([' '.join(review) for review in preprocessed_dataG])\n\n\nword_indexG = tokenizerG.word_index\n\n# Pad sequences\nmax_length = 300\npadded_data = pad_sequences(sequencesG, maxlen=max_length, padding='post')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:32:03.008243Z","iopub.execute_input":"2024-06-16T15:32:03.008704Z","iopub.status.idle":"2024-06-16T15:32:07.017860Z","shell.execute_reply.started":"2024-06-16T15:32:03.008667Z","shell.execute_reply":"2024-06-16T15:32:07.016232Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"  0%|          | 0/125 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n100%|██████████| 125/125 [00:01<00:00, 70.86it/s]\n0it [00:00, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1457\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mEmpty\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLSchemeUnknown\u001b[0m                          Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:633\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection_with_tls_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocationValueError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.get_connection_with_tls_context\u001b[0;34m(self, request, verify, proxies, cert)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Only scheme should be lower case\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoolmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection_from_host\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_kwargs\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/poolmanager.py:304\u001b[0m, in \u001b[0;36mPoolManager.connection_from_host\u001b[0;34m(self, host, port, scheme, pool_kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m request_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection_from_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_context\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/poolmanager.py:326\u001b[0m, in \u001b[0;36mPoolManager.connection_from_context\u001b[0;34m(self, request_context)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_key_constructor:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLSchemeUnknown(scheme)\n\u001b[1;32m    327\u001b[0m pool_key \u001b[38;5;241m=\u001b[39m pool_key_constructor(request_context)\n","\u001b[0;31mURLSchemeUnknown\u001b[0m: Not supported URL scheme http+docker","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/api/client.py:213\u001b[0m, in \u001b[0;36mAPIClient._retrieve_server_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApiVersion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/api/daemon.py:181\u001b[0m, in \u001b[0;36mDaemonApiMixin.version\u001b[0;34m(self, api_version)\u001b[0m\n\u001b[1;32m    180\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/version\u001b[39m\u001b[38;5;124m\"\u001b[39m, versioned_api\u001b[38;5;241m=\u001b[39mapi_version)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/utils/decorators.py:44\u001b[0m, in \u001b[0;36mupdate_headers.<locals>.inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_general_configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHttpHeaders\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/api/client.py:236\u001b[0m, in \u001b[0;36mAPIClient._get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;129m@update_headers\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_request_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:637\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocationValueError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_verify(conn, request\u001b[38;5;241m.\u001b[39murl, verify, cert)\n","\u001b[0;31mInvalidURL\u001b[0m: Not supported URL scheme http+docker","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mDockerException\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocessed_text\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocess all data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m preprocessed_dataG \u001b[38;5;241m=\u001b[39m [preprocess_german_text(review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m x_trainG]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenizer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tokenizerG \u001b[38;5;241m=\u001b[39m Tokenizer()\n","Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocessed_text\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocess all data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m preprocessed_dataG \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess_german_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m x_trainG]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenizer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tokenizerG \u001b[38;5;241m=\u001b[39m Tokenizer()\n","Cell \u001b[0;32mIn[52], line 5\u001b[0m, in \u001b[0;36mpreprocess_german_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_german_text\u001b[39m(text):\n\u001b[0;32m----> 5\u001b[0m     preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocessed_text\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/german/preprocessing.py:36\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(texts, n_jobs, remove_stop)\u001b[0m\n\u001b[1;32m     34\u001b[0m texts \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)(delayed(first_clean)(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(texts))\n\u001b[1;32m     35\u001b[0m texts \u001b[38;5;241m=\u001b[39m lemmatize(texts, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, remove_stop\u001b[38;5;241m=\u001b[39mremove_stop)\n\u001b[0;32m---> 36\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecond_clean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m texts\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1469\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m   1466\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1469\u001b[0m     islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1471\u001b[0m     \u001b[38;5;66;03m# Handle the fact that the generator of task raised an\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;66;03m# exception. As this part of the code can be executed in\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;66;03m# a thread internal to the backend, register a task with\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;66;03m# an error that will be raised in the user's thread.\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39m__context__, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1476\u001b[0m         \u001b[38;5;66;03m# Suppress the cause of the exception if it is\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m         \u001b[38;5;66;03m# queue.Empty to avoid cluttered traceback. Only do it\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m         \u001b[38;5;66;03m# if the __context__ is really empty to avoid messing\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m         \u001b[38;5;66;03m# with causes of the original error.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/german/preprocessing.py:36\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m texts \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)(delayed(first_clean)(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(texts))\n\u001b[1;32m     35\u001b[0m texts \u001b[38;5;241m=\u001b[39m lemmatize(texts, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, remove_stop\u001b[38;5;241m=\u001b[39mremove_stop)\n\u001b[0;32m---> 36\u001b[0m texts \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)(delayed(second_clean)(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(texts))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m texts\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/german_lemmatizer/lemmatize.py:77\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(texts, chunk_size, working_dir, escape, n_jobs, remove_stop)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\n\u001b[1;32m     74\u001b[0m     texts, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, working_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, escape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, remove_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     75\u001b[0m ):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# pull image if not present\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mdocker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     images_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([l\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mlist()], [])\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m docker_image_tag \u001b[38;5;129;01min\u001b[39;00m images_list:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/client.py:94\u001b[0m, in \u001b[0;36mDockerClient.from_env\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m version \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m use_ssh_client \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_ssh_client\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_pool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_pool_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ssh_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ssh_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_from_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/client.py:45\u001b[0m, in \u001b[0;36mDockerClient.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m=\u001b[39m \u001b[43mAPIClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/api/client.py:197\u001b[0m, in \u001b[0;36mAPIClient.__init__\u001b[0;34m(self, base_url, version, timeout, tls, user_agent, num_pools, credstore_env, use_ssh_client, max_pool_size)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# version detection needs to be after unix adapter mounting\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    194\u001b[0m                         version,\n\u001b[1;32m    195\u001b[0m                         \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    196\u001b[0m                         ) \u001b[38;5;129;01mand\u001b[39;00m version\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_server_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m version\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/docker/api/client.py:220\u001b[0m, in \u001b[0;36mAPIClient._retrieve_server_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DockerException(\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid response from docker daemon: key \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApiVersion\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is missing.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mke\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DockerException(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError while fetching server API version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    222\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mDockerException\u001b[0m: Error while fetching server API version: Not supported URL scheme http+docker"],"ename":"DockerException","evalue":"Error while fetching server API version: Not supported URL scheme http+docker","output_type":"error"}]},{"cell_type":"code","source":"# Example German sentence\ngerman_sentence = \"Ich habe den ganzen Tag Deutsch gelernt.\"\n\n# Preprocess the German sentence\npreprocessed_sentence = preprocess_german_text(german_sentence)\nprint(\"Preprocessed Sentence:\", preprocessed_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:56:04.571233Z","iopub.execute_input":"2024-06-16T13:56:04.571593Z","iopub.status.idle":"2024-06-16T13:56:04.597582Z","shell.execute_reply.started":"2024-06-16T13:56:04.571566Z","shell.execute_reply":"2024-06-16T13:56:04.596101Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Preprocessed Sentence: ich habe den ganzen tag deutsch gelernt .\n","output_type":"stream"}]},{"cell_type":"code","source":"padded_data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PSfs2TzoRaX","outputId":"ad042025-b648-4fc7-9105-3cf4acccf4c6","execution":{"iopub.status.busy":"2024-06-15T18:26:02.666546Z","iopub.execute_input":"2024-06-15T18:26:02.666854Z","iopub.status.idle":"2024-06-15T18:26:02.672624Z","shell.execute_reply.started":"2024-06-15T18:26:02.666829Z","shell.execute_reply":"2024-06-15T18:26:02.671793Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(34156, 200)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zicAOmu_Dsr0","outputId":"429b6343-e87e-4303-a728-cff9012d0468","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 300\nnum_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, embedding_dim)) #create an array of zeros with word_num rows and embedding_dim columns\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if i < num_words:\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector","metadata":{"id":"_QWnOm3H_Gsn","execution":{"iopub.status.busy":"2024-06-15T18:26:02.673605Z","iopub.execute_input":"2024-06-15T18:26:02.673875Z","iopub.status.idle":"2024-06-15T18:26:02.960609Z","shell.execute_reply.started":"2024-06-15T18:26:02.673852Z","shell.execute_reply":"2024-06-15T18:26:02.959804Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qy7UQ2euoIkw","outputId":"d0f7f143-1022-4ecc-b00d-c487b98b8a93","execution":{"iopub.status.busy":"2024-06-15T18:26:02.962801Z","iopub.execute_input":"2024-06-15T18:26:02.963094Z","iopub.status.idle":"2024-06-15T18:26:02.970082Z","shell.execute_reply.started":"2024-06-15T18:26:02.963069Z","shell.execute_reply":"2024-06-15T18:26:02.969215Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(86447, 300)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"-xbRzg4_EZ8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\n# Reshape y_train to a column vector (required by OneHotEncoder)\ny_train =np.array( y_train)\ny_train=y_train.reshape(-1, 1)\n\n# Create an instance of OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n\n# Fit and transform y_train to one-hot encoded format\ny_train_onehot = encoder.fit_transform(y_train)\nprint(y_train_onehot[0])\n\n","metadata":{"id":"E41RmnRt_KLF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"73544216-59ce-4269-c4f8-a10072a1d167","execution":{"iopub.status.busy":"2024-06-16T13:56:41.965132Z","iopub.execute_input":"2024-06-16T13:56:41.965481Z","iopub.status.idle":"2024-06-16T13:56:41.987177Z","shell.execute_reply.started":"2024-06-16T13:56:41.965455Z","shell.execute_reply":"2024-06-16T13:56:41.985563Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[0. 0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_9C1Rqe9TG3","outputId":"883edf83-9b18-4635-b7d7-07d4fed37bb3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pos_enc + glove_emb","metadata":{"id":"HAOwy3lArZ8k"}},{"cell_type":"code","source":"","metadata":{"id":"28ZLQZm9rZWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"self attention mech","metadata":{"id":"CXKMI_KZrwz8"}},{"cell_type":"code","source":"def softmax(matrix):\n    # Subtract the max value from each row for numerical stability\n    exp_matrix = np.exp(matrix - np.max(matrix, axis=1, keepdims=True))\n    # Divide by the sum of exponentials for each row\n    softmax_matrix = exp_matrix / np.sum(exp_matrix, axis=1, keepdims=True)\n    return softmax_matrix","metadata":{"id":"-HkxPMjD3Omu","execution":{"iopub.status.busy":"2024-06-15T18:26:02.999709Z","iopub.execute_input":"2024-06-15T18:26:02.999976Z","iopub.status.idle":"2024-06-15T18:26:03.004936Z","shell.execute_reply.started":"2024-06-15T18:26:02.999954Z","shell.execute_reply":"2024-06-15T18:26:03.003992Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"For weight matrices Q,K,V we are using Dense layers as they are trainable","metadata":{"id":"KjdrSUrR26w1"}},{"cell_type":"code","source":"x = np.random.rand(2,5,100) # u can send in batches\nW = Dense(32,use_bias=False) # bias should not be there as these act only as matrices with weights even no activation funcs\nQ = Dense(32,use_bias=False)\nK = Dense(32,use_bias=False)\nw= W(x)\nQ(x)\nK(x)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0Xuf9TUAZUm","outputId":"c0b083ad-e153-4929-d34a-3b2c1af7af87","execution":{"iopub.status.busy":"2024-06-15T18:26:03.005836Z","iopub.execute_input":"2024-06-15T18:26:03.006103Z","iopub.status.idle":"2024-06-15T18:26:03.757164Z","shell.execute_reply.started":"2024-06-15T18:26:03.006081Z","shell.execute_reply":"2024-06-15T18:26:03.756219Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 5, 32), dtype=float32, numpy=\narray([[[ 1.3476851 ,  2.1903548 , -0.22787488, -0.06536621,\n          0.14794889, -0.25170103,  0.26709646,  0.17225453,\n         -0.13360104, -0.29655674,  1.0847025 ,  0.24041533,\n          0.7929813 , -0.10451432, -1.1481694 ,  1.3591585 ,\n          0.9238999 ,  0.40648383, -1.6882964 ,  0.967045  ,\n          0.6605883 ,  0.22796515,  1.2884505 , -0.62256664,\n          0.15587823,  0.98689127, -0.734223  , -0.43940696,\n         -0.5137866 , -1.3471773 , -0.31607616, -0.39597237],\n        [ 1.2252619 ,  2.2225633 , -0.9059501 , -0.48237148,\n          0.7914054 , -0.09414245,  0.8324181 ,  0.7702593 ,\n         -0.9307699 , -0.8684033 ,  0.41811052,  0.34334654,\n          1.0489726 ,  0.9834802 , -1.631293  ,  0.7341723 ,\n          0.38364398,  1.3851092 , -1.1220211 ,  1.0967622 ,\n          0.0278936 ,  0.8224919 ,  1.1516116 , -0.05503222,\n         -0.07610722,  0.6356843 , -0.78698903, -0.78363067,\n          0.44623363, -1.340215  ,  0.37333968, -0.43778217],\n        [ 0.7557477 ,  2.3254857 , -1.0468059 , -0.406173  ,\n          0.10149761,  0.2712741 , -0.11815408,  0.08217721,\n         -0.59649676, -0.5762272 ,  1.0813446 ,  0.92993504,\n          0.8081366 ,  0.09205019, -0.9310896 ,  0.8957236 ,\n          0.5092143 ,  0.9824515 , -0.98415005,  1.2002267 ,\n          0.1716676 ,  0.5072318 ,  1.1624712 , -0.3931576 ,\n          0.29862726,  0.6308649 , -0.6385889 , -0.4743146 ,\n         -0.2726227 , -1.0663955 , -0.3591835 , -0.78381085],\n        [ 1.2275461 ,  2.1059372 , -0.93295866, -0.26366308,\n          0.6613848 ,  0.11070991, -0.05208664, -0.4910168 ,\n         -0.06516591,  0.07922482,  0.49306875,  0.7419104 ,\n          0.72636986, -0.30041713, -0.7613998 ,  0.6775573 ,\n          0.8182845 ,  0.82954246, -1.3009405 ,  1.0991064 ,\n          0.5720939 ,  0.06193593,  1.0988356 ,  0.01999184,\n          0.06173748,  0.9413225 , -0.92132336, -1.2178714 ,\n          0.5663029 , -1.8611386 ,  0.43547508, -1.1793175 ],\n        [ 0.4745668 ,  2.0213804 , -0.8960492 ,  0.17076212,\n          0.15407237, -0.36248088,  0.16900788,  0.1801434 ,\n          0.144899  , -0.4865942 ,  1.2186859 ,  0.438413  ,\n          0.6550203 ,  0.22353503, -0.77502286,  0.3525794 ,\n          0.46644688,  1.4660609 , -0.8775818 ,  0.9587352 ,\n          0.16773926,  0.60512   ,  0.5770625 , -0.8176023 ,\n          0.04014891,  0.5778069 , -1.1605585 , -0.9278447 ,\n          0.4162898 , -1.5287197 ,  0.45167327,  0.02575046]],\n\n       [[ 0.99688596,  1.7858057 , -0.9382834 , -1.02994   ,\n          0.96852815, -0.8433379 ,  0.37703615, -0.25529373,\n         -0.733277  , -0.45803767,  0.5003494 ,  0.7082691 ,\n          0.4637883 ,  0.19600695, -0.7976029 ,  0.5744618 ,\n          0.21998626,  1.1670943 , -1.1521502 ,  0.85400987,\n          0.70159286,  0.1295585 ,  1.2307767 ,  0.07157068,\n          0.41441637,  0.17154083, -0.9058761 , -0.8459258 ,\n          0.5149823 , -1.5701634 ,  0.43131104, -0.3302095 ],\n        [ 0.95765007,  1.6017659 , -1.2622911 , -0.23963869,\n          0.31567344, -1.000607  ,  0.45835054, -0.16311802,\n         -0.20459135, -0.14239119,  0.8808023 , -0.42832154,\n          0.68552953,  0.11780978, -1.1944541 ,  0.7288577 ,\n          0.553751  ,  1.1043278 , -1.4631238 ,  0.616415  ,\n          0.85116464,  0.48435834,  1.0535522 , -0.38087344,\n         -0.497306  ,  0.9527657 , -0.82676005, -0.49894574,\n          0.72167176, -1.3142593 ,  0.1711981 , -0.26378128],\n        [ 1.2952175 ,  2.2650921 , -0.24200366, -0.2935389 ,\n          0.46820897, -0.55265766, -0.2470859 ,  0.26115566,\n          0.05711153, -0.2634122 ,  0.5801039 ,  1.0063055 ,\n         -0.05994719, -0.13103451, -0.91405797,  0.26954347,\n          1.0651654 ,  0.867133  , -1.1965483 ,  0.8118892 ,\n          0.6881542 ,  0.6276292 ,  1.2420442 ,  0.3329544 ,\n         -0.01366261,  0.4096027 ,  0.02260293, -0.9618069 ,\n          0.95156455, -1.4796584 , -0.04829952, -1.0066544 ],\n        [ 1.2500489 ,  2.0592425 , -1.1857641 , -0.96319485,\n          1.263107  ,  0.37698215,  0.32707566,  0.40099466,\n         -0.30103222, -0.25726196,  0.44612348,  0.45076466,\n          0.7073647 ,  0.19486926, -0.9476296 ,  0.9519328 ,\n          1.2638773 ,  1.0555385 , -0.88570106,  0.96283656,\n          0.7849121 ,  0.8206488 ,  1.0341676 , -0.24637266,\n          0.16507919,  0.9769932 , -0.23379362, -1.053487  ,\n          0.74320185, -1.1451524 ,  0.2756586 , -0.22553584],\n        [ 0.86021596,  2.309724  , -1.4663291 , -0.47008127,\n          0.771281  , -0.37836495,  0.67315197,  0.16506512,\n         -0.3690263 ,  0.17486751,  1.008331  ,  0.627935  ,\n          0.14525792,  0.337229  , -1.1618453 ,  0.6516019 ,\n          0.10117872,  0.9631819 , -1.4926436 ,  1.3079832 ,\n          0.8184048 ,  0.6006977 ,  0.8042669 , -0.6504837 ,\n         -0.235161  ,  1.0460955 , -1.4564965 , -1.0158712 ,\n          0.15081915, -1.7184596 ,  0.2680881 , -0.7696591 ]]],\n      dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"V = Dense(32,use_bias=False) # bias should not be there as these act only as matrices with weights even no activation funcs\nQ = Dense(32,use_bias=False)\nK = Dense(32,use_bias=False)\n\ndef self_attention(x,Q,K,V):\n  c=x.shape[0]\n  k = K(x)\n  q = Q(x)\n  v = V(x)\n  att = np.zeros((c,c))\n\n  att = np.matmul(q,tf.transpose(k))\n  att = att/np.sqrt(x.shape[1])\n  att = softmax(att)\n\n  # need to apply softmax while next step x as 2d mat\n  w = np.matmul(att,v)\n  return w","metadata":{"id":"TurJSfRur2q0","execution":{"iopub.status.busy":"2024-06-15T18:26:03.758484Z","iopub.execute_input":"2024-06-15T18:26:03.758887Z","iopub.status.idle":"2024-06-15T18:26:03.769445Z","shell.execute_reply.started":"2024-06-15T18:26:03.758854Z","shell.execute_reply":"2024-06-15T18:26:03.768526Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\nx = np.random.rand(2,20,10)\ny =Dense(15)\nz = y(x)\nz1 = y(x)\n\n#z2 = tf.matmul(z1,z,transpose_b=False)\n# x3 = tf.concat([z, z], axis=1)\n# x4= LayerNormalization()\n# x=x4(x+x)\n# x.shape","metadata":{"id":"Mtswq5Pl4qRr","execution":{"iopub.status.busy":"2024-06-15T18:26:03.770454Z","iopub.execute_input":"2024-06-15T18:26:03.770772Z","iopub.status.idle":"2024-06-15T18:26:03.805957Z","shell.execute_reply.started":"2024-06-15T18:26:03.770727Z","shell.execute_reply":"2024-06-15T18:26:03.805169Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"creating model\n","metadata":{"id":"0SmQbWrX4oek"}},{"cell_type":"code","source":"# Including the matrices and the function in single class\n\nclass SelfAttention(Layer):\n    def __init__(self, embS):     #id --> input dim   embS --> matrice dim\n        super(SelfAttention, self).__init__()\n        self.embed_size = embS\n        self.Q = Dense(embS, use_bias=False)\n        self.K = Dense(embS, use_bias=False)\n        self.V = Dense(embS, use_bias=False)\n\n    def call(self, x):\n        q = self.Q(x)\n        k = self.K(x)\n        v = self.V(x)\n\n        att = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.embed_size, tf.float32))\n        att_weights = tf.nn.softmax(att, axis=-1)\n        output = tf.matmul(att_weights, v)\n\n        return output\n\n        \"\"\"When subclassing tf.keras.Model, you need to define a call() method explicitly. This method specifies how inputs are processed through the model during both training and inference\"\"\"","metadata":{"id":"kYQcDLkD3-53","execution":{"iopub.status.busy":"2024-06-16T17:06:36.624736Z","iopub.execute_input":"2024-06-16T17:06:36.625128Z","iopub.status.idle":"2024-06-16T17:06:36.633533Z","shell.execute_reply.started":"2024-06-16T17:06:36.625097Z","shell.execute_reply":"2024-06-16T17:06:36.632643Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"# Define the model with self-attention\n\nclass AttentionModel(Model):\n    def __init__(self, embed_size):\n        super(AttentionModel, self).__init__()\n        self.attention = SelfAttention(embed_size)\n\n    def call(self, x):\n        x = self.attention(x)\n        x = tf.reduce_mean(x, axis=1)  # Pooling\n        return x\n","metadata":{"id":"EPlP1CHg_1BE","execution":{"iopub.status.busy":"2024-06-16T17:06:38.958044Z","iopub.execute_input":"2024-06-16T17:06:38.958422Z","iopub.status.idle":"2024-06-16T17:06:38.964374Z","shell.execute_reply.started":"2024-06-16T17:06:38.958391Z","shell.execute_reply":"2024-06-16T17:06:38.963361Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"markdown","source":"Creating multihead attention","metadata":{"id":"K6lILMwc-clA"}},{"cell_type":"code","source":"#inheriting selfattention model\nclass MultiHead(AttentionModel):\n  def __init__(self,embS,id,heads=2):\n    super(MultiHead,self).__init__(embS)\n    self.att = [SelfAttention(embS) for _ in range(heads) ]  # multiple self att instances\n    self.dense = Dense(id, use_bias = False)\n\n  def call(self,x):\n    outs = [ f(x) for f in self.att ]   # x--> inputs\n\n    x3 = tf.concat(outs , axis=-1)\n    x3 = self.dense(x3)\n    return x3\n\n    \"\"\"Working for single sentence of any length  \"\"\"\n\n","metadata":{"id":"xwWq5VmD-f3u","execution":{"iopub.status.busy":"2024-06-16T17:06:42.071603Z","iopub.execute_input":"2024-06-16T17:06:42.071971Z","iopub.status.idle":"2024-06-16T17:06:42.079045Z","shell.execute_reply.started":"2024-06-16T17:06:42.071943Z","shell.execute_reply":"2024-06-16T17:06:42.077998Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"markdown","source":"Add&Norm","metadata":{"id":"VPCWJhF2JbJk"}},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization\nfrom keras.models import Model\nfrom keras.initializers import Constant\n\n# Define the custom Positional Encoding Layer\nclass PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, position, d_model):\n        super(PositionalEncoding, self).__init__()\n        self.pos_encoding = self.positional_encoding(position, d_model)\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'position': self.pos_encoding.shape[0],\n            'd_model': self.pos_encoding.shape[1],\n        })\n        return config\n\n    def positional_encoding(self, position, d_model):\n        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n                                     np.arange(d_model)[np.newaxis, :],\n                                     d_model)\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n        pos_encoding = angle_rads[np.newaxis, ...]\n        return tf.cast(pos_encoding, dtype=tf.float32)\n\n    def get_angles(self, pos, i, d_model):\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        return pos * angle_rates\n\n    def call(self, inputs):\n        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n\n\n\n\n","metadata":{"id":"q1fE3M62JZH4","execution":{"iopub.status.busy":"2024-06-16T17:06:44.687539Z","iopub.execute_input":"2024-06-16T17:06:44.688423Z","iopub.status.idle":"2024-06-16T17:06:44.699419Z","shell.execute_reply.started":"2024-06-16T17:06:44.688389Z","shell.execute_reply":"2024-06-16T17:06:44.698485Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"def pos_encoding(max,input,d):\n  pos_enc = np.zeros((max,d))\n  c=int(d/2)\n  for pos in range(0,max):\n    for i in range(0, c):\n      pos_enc[pos,2*i] = x[pos,2*i]+np.sin(pos/(10000**( (2*i)/d)))\n      pos_enc[pos,2*i + 1] = x[pos,(2*i)+1]+np.cos(pos/(10000**( (2*i)/d)))\n\n  return pos_enc","metadata":{"id":"Xd6ryNu3JRiX","execution":{"iopub.status.busy":"2024-06-15T18:26:03.854495Z","iopub.execute_input":"2024-06-15T18:26:03.854783Z","iopub.status.idle":"2024-06-15T18:26:03.867676Z","shell.execute_reply.started":"2024-06-15T18:26:03.854734Z","shell.execute_reply":"2024-06-15T18:26:03.866798Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"len(word_index)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:58:58.486817Z","iopub.execute_input":"2024-06-16T13:58:58.487515Z","iopub.status.idle":"2024-06-16T13:58:58.493849Z","shell.execute_reply.started":"2024-06-16T13:58:58.487487Z","shell.execute_reply":"2024-06-16T13:58:58.492921Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"86446"},"metadata":{}}]},{"cell_type":"code","source":"from keras.losses import CategoricalCrossentropy\nfrom keras.layers import Embedding,GlobalAveragePooling1D,Dropout\nfrom keras.optimizers import Adam\n\n# Hyperparameters\nembS = 16\nbatch_size = 100\nheads = 2\nmax_length = 200\nid=300\nembedding_dim = id\nnum_words = len(word_index)+1\n\nclass encoder(Model):\n  def __init__(self,max_length,num_words,embedding_dim,id,embS,heads):   #embS --> matrix embed size\n    super(encoder,self).__init__()\n    #self.inputs = tf.keras.Input(shape=(max_length,))\n#     self.embeds = Embedding(input_dim=num_words ,\n#                     output_dim=embedding_dim,\n#                     weights=[embedding_matrix],\n#                     input_length=max_length,\n#                     trainable=False)\n    self.le = Embedding(num_words,embedding_dim )\n    self.pos_enc = PositionalEncoding(max_length, id)\n    self.drop = Dropout(0.1)\n    self.multiH = MultiHead(embS,id,heads)\n    self.layer1 = LayerNormalization()\n    self.dense1 = Dense(4*id,activation='relu')\n    self.dense2 = Dense(id)\n    self.layer2 = LayerNormalization()\n    self.outputs = GlobalAveragePooling1D()\n    #self.cc = Dense(3, activation='softmax')\n\n\n  def call(self,x):\n    #inputs = self.inputs(x)\n\n    emb=  self.le(x)\n\n    poser = self.pos_enc(emb)\n    poser = self.drop(poser)\n    multi =  self.multiH(poser)\n    d1 = self.drop(poser + multi)\n    AN1   =  self.layer1(d1)\n    dense1 =  self.dense1(AN1)\n    dense2 =  self.dense2(dense1)\n    d2 = self.drop(AN1 + dense2)\n    outputs1   =  self.layer2(d2)\n    #outputM = self.outputs(outputs1)\n    #outputcc = self.cc(outputM)\n\n    return outputs1\n\n# Final Output Layer\n\n\nenc_model = encoder(max_length,num_words,embedding_dim,id,embS,heads)\n\nadam_optimizer = Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)\nenc_model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])          ## Sparse categorical entropy FUCKS here\n\n","metadata":{"id":"ajWb4kWF_7ew","execution":{"iopub.status.busy":"2024-06-16T17:06:49.892176Z","iopub.execute_input":"2024-06-16T17:06:49.892609Z","iopub.status.idle":"2024-06-16T17:06:49.939546Z","shell.execute_reply.started":"2024-06-16T17:06:49.892557Z","shell.execute_reply":"2024-06-16T17:06:49.938596Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"def preprocess_and_pad(text, max_length=200, use_stemming=False):\n    # Tokenization\n    tokens = word_tokenize(text)\n\n    # Convert to Lowercase\n    tokens = [word.lower() for word in tokens]\n\n    # Remove Punctuation\n    tokens = [word for word in tokens if word.isalpha()]\n\n\n\n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n    # Stemming (optional)\n    if use_stemming:\n        stemmer = PorterStemmer()\n        tokens = [stemmer.stem(word) for word in tokens]\n\n    # Convert tokens to integer indices\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts([' '.join(tokens)])\n    sequences = tokenizer.texts_to_sequences([' '.join(tokens)])\n\n    # Pad sequences\n    max_length = 200\n    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n    return padded_sequences\n","metadata":{"id":"Q_Q6GsnfYhLa","execution":{"iopub.status.busy":"2024-06-15T19:28:39.713803Z","iopub.execute_input":"2024-06-15T19:28:39.714160Z","iopub.status.idle":"2024-06-15T19:28:39.722000Z","shell.execute_reply.started":"2024-06-15T19:28:39.714131Z","shell.execute_reply":"2024-06-15T19:28:39.721058Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"stage-1 training #Done !!!! #","metadata":{"id":"89EnX5fTr52k"}},{"cell_type":"markdown","source":"Decoder Starts","metadata":{"id":"1rg1uF8vjGAp"}},{"cell_type":"markdown","source":"#1 Crossed multi","metadata":{"id":"O7tdedZQmPzl"}},{"cell_type":"code","source":"# Including the matrices and the function in single class\n\nclass CrossSelfAttention(Layer):\n    def __init__(self, embS):     # q --> output of masked multi\n        super(CrossSelfAttention, self).__init__()\n        self.embed_size = embS\n        self.K = Dense(embS, use_bias=False)\n        self.V = Dense(embS, use_bias=False)\n        self.Q = Dense(embS, use_bias=False)\n\n    def call(self, x):                # 1--> masked multi output   0--> encoder output\n        q = self.Q(x[1])\n        k = self.K(x[0])\n        v = self.V(x[0])\n\n        att = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.embed_size, tf.float32))\n        att_weights = tf.nn.softmax(att, axis=-1)\n        output = tf.matmul(att_weights, v)\n\n        return output\n\n        \"\"\"When subclassing tf.keras.Model, you need to define a call() method explicitly. This method specifies how inputs are processed through the model during both training and inference\"\"\"","metadata":{"id":"xkj0z2VLkCmq","execution":{"iopub.status.busy":"2024-06-16T17:06:55.962056Z","iopub.execute_input":"2024-06-16T17:06:55.962822Z","iopub.status.idle":"2024-06-16T17:06:55.971049Z","shell.execute_reply.started":"2024-06-16T17:06:55.962785Z","shell.execute_reply":"2024-06-16T17:06:55.970070Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"class CrossedMulti(CrossSelfAttention):\n  def __init__(self,embS,id,heads=2):      #q-->encoder output     id--> input word dimension   embS --> embed dimension\n    super(CrossedMulti,self).__init__(embS)\n    self.att = [CrossSelfAttention(embS) for _ in range(heads) ]  # multiple self att instances\n    self.dense = Dense(id, use_bias = False)\n\n  def call(self,x):\n    outs = [ f(x) for f in self.att ]   # x--> inputs\n\n    x3 = tf.concat(outs , axis=-1)\n    x3 = self.dense(x3)\n    return x3\n","metadata":{"id":"Jg4SNOEfXZQ2","execution":{"iopub.status.busy":"2024-06-16T17:06:58.250105Z","iopub.execute_input":"2024-06-16T17:06:58.250840Z","iopub.status.idle":"2024-06-16T17:06:58.258818Z","shell.execute_reply.started":"2024-06-16T17:06:58.250806Z","shell.execute_reply":"2024-06-16T17:06:58.257652Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"markdown","source":"#2 Masked multi","metadata":{"id":"2P7KgdRnmTST"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.layers import Dense, Layer\nimport numpy as np\n\nclass MaskedSelfAttention(Layer):\n    def __init__(self, emb, nw, wd):  # wd is ideally 64\n        super(MaskedSelfAttention, self).__init__()\n        self.embed_size = emb\n        self.nw = nw\n        self.wd = wd\n        self.K = Dense(emb, use_bias=False)\n        self.V = Dense(emb, use_bias=False)\n        self.Q = Dense(emb, use_bias=False)\n\n    def masker(self):\n        mask = np.zeros((self.nw, self.nw))\n        for i in range(self.nw):\n            for j in range(i + 1, self.nw):\n                mask[i, j] = -np.inf\n        return mask\n\n    def call(self, x):\n        q = self.Q(x)\n        k = self.K(x)\n        v = self.V(x)\n        M = self.masker()\n        att = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.embed_size, tf.float32))\n        M = tf.cast(tf.convert_to_tensor(M),dtype = tf.float32)\n        att = att + M\n        att_weights = tf.nn.softmax(att, axis=-1)\n        output = tf.matmul(att_weights, v)\n        return output\n","metadata":{"id":"iiYd-MLsmWLs","execution":{"iopub.status.busy":"2024-06-16T17:07:00.555993Z","iopub.execute_input":"2024-06-16T17:07:00.556856Z","iopub.status.idle":"2024-06-16T17:07:00.567044Z","shell.execute_reply.started":"2024-06-16T17:07:00.556825Z","shell.execute_reply":"2024-06-16T17:07:00.566037Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"class Maskedmulti(MaskedSelfAttention):\n    def __init__(self, emb, nw, wd, heads=2):\n        super(Maskedmulti, self).__init__(emb, nw, wd)\n        self.heads = heads\n        self.att = [MaskedSelfAttention(emb, nw, wd) for _ in range(heads)]\n        self.dense = Dense(wd, use_bias=False)\n\n    def call(self, x):\n        outs = [head(x) for head in self.att]\n        x3 = tf.concat(outs, axis=-1)\n        x3 = self.dense(x3)\n        return x3\n","metadata":{"id":"0vz_plgk9NhJ","execution":{"iopub.status.busy":"2024-06-16T17:07:03.763860Z","iopub.execute_input":"2024-06-16T17:07:03.764810Z","iopub.status.idle":"2024-06-16T17:07:03.773064Z","shell.execute_reply.started":"2024-06-16T17:07:03.764769Z","shell.execute_reply":"2024-06-16T17:07:03.772028Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"markdown","source":"Time to pack everything","metadata":{"id":"2BEZNk9i-Jdx"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.layers import Embedding, Dense, LayerNormalization, GlobalAveragePooling1D\n\nclass decoderModel(tf.keras.Model):\n    def __init__(self ,num_words, embedding_dim,max_length, Dembd, nw, wd, heads): # embd --> matrixD in transformers wd--> wordD  nw -->max_length\n        super(decoderModel, self).__init__()\n#         self.embedding_layer = Embedding(input_dim=num_words,\n#                                          output_dim=embedding_dim,\n#                                          weights=[embedding_matrix],\n#                                          input_length=max_length,\n#                                          trainable=False)\n        self.le = Embedding(num_words,embedding_dim)\n        self.positional_encoding = PositionalEncoding(position=nw, d_model=wd)\n        self.maskedmulti_layer = Maskedmulti(Dembd,nw, wd, heads)\n        self.crossedmulti_layer = CrossedMulti(Dembd, wd, heads)\n        self.layer_norm1 = LayerNormalization()\n        self.layer_norm2 = LayerNormalization()\n        self.dense1 = Dense(4*wd, activation='relu')\n        self.dense2 = Dense(wd, activation='relu')\n        self.global_avg_pooling = GlobalAveragePooling1D()\n        self.dense3 = Dense(1,activation = 'linear')\n        self.drop = Dropout(0.1)\n\n    def call(self, inputs):\n        emb = self.le(inputs[0])\n        poser = self.positional_encoding(emb)\n        maskmulti = self.maskedmulti_layer(poser)\n        d1 = self.drop(emb + maskmulti)\n        SL1 = self.layer_norm1(d1)\n        cross = self.crossedmulti_layer([SL1,inputs[1]])\n        d2 = self.drop(SL1 + cross)\n        SL2 = self.layer_norm2(d2)\n        dense1_out = self.dense1(SL2)\n        dense2_out = self.dense2(dense1_out)\n        d3 = self.drop(SL2 + dense2_out)\n        SL3 = self.layer_norm2(d3)\n        #outputM = self.global_avg_pooling(SL3)\n        output = self.dense3(SL3)\n        output = tf.transpose(output, perm=[0,2,1])\n\n        return output\n","metadata":{"id":"CxNaQcE5_NcS","execution":{"iopub.status.busy":"2024-06-16T17:07:06.062644Z","iopub.execute_input":"2024-06-16T17:07:06.063022Z","iopub.status.idle":"2024-06-16T17:07:06.075205Z","shell.execute_reply.started":"2024-06-16T17:07:06.062991Z","shell.execute_reply":"2024-06-16T17:07:06.073952Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"markdown","source":"combining two classes into transformer","metadata":{"id":"4Wb4985B3xo1"}},{"cell_type":"code","source":"class Transformer(Model):\n  def __init__(self,encoder,decoder):\n    super(Transformer,self).__init__()\n    super(Transformer).__init__()\n    self.enc = encoder\n    self.dec = decoder\n\n  def call(self,inputs):\n    enc1 = self.enc(inputs[0])\n    dec1 = self.dec([inputs[1],enc1])\n\n    return dec1\n","metadata":{"id":"UFmcA2kG31id","execution":{"iopub.status.busy":"2024-06-16T17:07:11.374701Z","iopub.execute_input":"2024-06-16T17:07:11.375152Z","iopub.status.idle":"2024-06-16T17:07:11.381382Z","shell.execute_reply.started":"2024-06-16T17:07:11.375124Z","shell.execute_reply":"2024-06-16T17:07:11.380450Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"max_length=len(word_index)+1","metadata":{"execution":{"iopub.status.busy":"2024-06-16T17:07:15.277182Z","iopub.execute_input":"2024-06-16T17:07:15.277551Z","iopub.status.idle":"2024-06-16T17:07:15.282026Z","shell.execute_reply.started":"2024-06-16T17:07:15.277521Z","shell.execute_reply":"2024-06-16T17:07:15.281049Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"code","source":"last_elements=np.array(last_elements,dtype=np.int32)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:44:30.447060Z","iopub.execute_input":"2024-06-16T16:44:30.447494Z","iopub.status.idle":"2024-06-16T16:44:30.452870Z","shell.execute_reply.started":"2024-06-16T16:44:30.447446Z","shell.execute_reply":"2024-06-16T16:44:30.451888Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"last_elements.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:41:03.330614Z","iopub.execute_input":"2024-06-16T16:41:03.330986Z","iopub.status.idle":"2024-06-16T16:41:03.337525Z","shell.execute_reply.started":"2024-06-16T16:41:03.330957Z","shell.execute_reply":"2024-06-16T16:41:03.336494Z"},"trusted":true},"execution_count":135,"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"(5000,)"},"metadata":{}}]},{"cell_type":"code","source":"embS = 64\nbatch_size = 100\nheads =4\nnw = 300\nwd =512\nid = 512\nembedding_dim=512\nnum_words = nw\n\nenc = encoder(max_length,num_words,embedding_dim,id,embS,heads)\ndec1 = decoderModel(num_words, embedding_dim,max_length,embS , nw , id , heads )\n\nmodel = Transformer(enc,dec1)\n\nadam_optimizer = Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)\nmodel.compile(optimizer=adam_optimizer, loss='mse', metrics=['accuracy'])\n\n\n\nmodel.fit([X_train,X_trainD],X_train,epochs =100000)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tK8kZZjS-I0V","outputId":"b336db6b-8145-42fa-adf2-472e77c6d4dd","execution":{"iopub.status.busy":"2024-06-16T19:13:21.463410Z","iopub.execute_input":"2024-06-16T19:13:21.463988Z","iopub.status.idle":"2024-06-16T19:13:21.906623Z","shell.execute_reply.started":"2024-06-16T19:13:21.463950Z","shell.execute_reply":"2024-06-16T19:13:21.905139Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      8\u001b[0m num_words \u001b[38;5;241m=\u001b[39m nw\n\u001b[0;32m---> 10\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m(max_length,num_words,embedding_dim,\u001b[38;5;28mid\u001b[39m,embS,heads)\n\u001b[1;32m     11\u001b[0m dec1 \u001b[38;5;241m=\u001b[39m decoderModel(num_words, embedding_dim,max_length,embS , nw , \u001b[38;5;28mid\u001b[39m , heads )\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(enc,dec1)\n","\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"],"ename":"NameError","evalue":"name 'encoder' is not defined","output_type":"error"}]},{"cell_type":"code","source":"y = model.predict([X_train[:3],X_trainD[:3]])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:50:21.189250Z","iopub.execute_input":"2024-06-16T16:50:21.190015Z","iopub.status.idle":"2024-06-16T16:50:23.093264Z","shell.execute_reply.started":"2024-06-16T16:50:21.189980Z","shell.execute_reply":"2024-06-16T16:50:23.092330Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","output_type":"stream"}]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:50:28.313051Z","iopub.execute_input":"2024-06-16T16:50:28.313973Z","iopub.status.idle":"2024-06-16T16:50:28.320176Z","shell.execute_reply.started":"2024-06-16T16:50:28.313929Z","shell.execute_reply":"2024-06-16T16:50:28.319285Z"},"trusted":true},"execution_count":155,"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"(3, 1, 300)"},"metadata":{}}]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:50:48.405272Z","iopub.execute_input":"2024-06-16T16:50:48.405664Z","iopub.status.idle":"2024-06-16T16:50:48.412023Z","shell.execute_reply.started":"2024-06-16T16:50:48.405633Z","shell.execute_reply":"2024-06-16T16:50:48.410980Z"},"trusted":true},"execution_count":156,"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"(5000, 300)"},"metadata":{}}]},{"cell_type":"code","source":"model.save_weights('main1q.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T19:09:14.994913Z","iopub.execute_input":"2024-06-15T19:09:14.995266Z","iopub.status.idle":"2024-06-15T19:09:15.616159Z","shell.execute_reply.started":"2024-06-15T19:09:14.995237Z","shell.execute_reply":"2024-06-15T19:09:15.615105Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T19:12:07.625547Z","iopub.execute_input":"2024-06-16T19:12:07.626295Z","iopub.status.idle":"2024-06-16T19:12:07.649483Z","shell.execute_reply.started":"2024-06-16T19:12:07.626262Z","shell.execute_reply":"2024-06-16T19:12:07.648648Z"},"trusted":true},"execution_count":197,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer_17\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_17\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ encoder_19 (\u001b[38;5;33mencoder\u001b[0m)            │ ?                      │     \u001b[38;5;34m2,779,648\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder_model_17 (\u001b[38;5;33mdecoderModel\u001b[0m) │ ?                      │     \u001b[38;5;34m3,304,449\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ encoder_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">encoder</span>)            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,779,648</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder_model_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">decoderModel</span>) │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,449</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,252,293\u001b[0m (69.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,252,293</span> (69.63 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,084,097\u001b[0m (23.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,084,097</span> (23.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m12,168,196\u001b[0m (46.42 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,168,196</span> (46.42 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-15T19:10:03.675440Z","iopub.execute_input":"2024-06-15T19:10:03.675810Z","iopub.status.idle":"2024-06-15T19:10:03.683741Z","shell.execute_reply.started":"2024-06-15T19:10:03.675784Z","shell.execute_reply":"2024-06-15T19:10:03.682877Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"(34156, 200)"},"metadata":{}}]},{"cell_type":"code","source":"from keras.models import Model\n\n# Remove the output layer from the original model\ninputs = Input(shape=(200,))\n\nop = model.enc(inputs)\nop1 = GlobalAveragePooling1D()(op)\n\nmodel2 = Model(inputs=inputs, outputs=op1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T19:25:44.556675Z","iopub.execute_input":"2024-06-15T19:25:44.557013Z","iopub.status.idle":"2024-06-15T19:25:44.658655Z","shell.execute_reply.started":"2024-06-15T19:25:44.556988Z","shell.execute_reply":"2024-06-15T19:25:44.657723Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"t1= 'Angle is danger'\nt2= 'Devil'\nx1 = preprocess_and_pad(t1)\nx2 = preprocess_and_pad(t2)\ny1= model2.predict(x1)\ny2 = model2.predict(x2)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T19:32:44.760617Z","iopub.execute_input":"2024-06-15T19:32:44.761487Z","iopub.status.idle":"2024-06-15T19:32:44.875004Z","shell.execute_reply.started":"2024-06-15T19:32:44.761456Z","shell.execute_reply":"2024-06-15T19:32:44.874250Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"y1-y2","metadata":{"execution":{"iopub.status.busy":"2024-06-15T19:32:47.807308Z","iopub.execute_input":"2024-06-15T19:32:47.807630Z","iopub.status.idle":"2024-06-15T19:32:47.816562Z","shell.execute_reply.started":"2024-06-15T19:32:47.807605Z","shell.execute_reply":"2024-06-15T19:32:47.815517Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.4901161e-08,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00, -3.7252903e-09,  0.0000000e+00,\n         0.0000000e+00,  1.4901161e-08,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  1.8626451e-09,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00, -1.4901161e-08,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  1.4901161e-08,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n        -1.4901161e-08,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  9.3132257e-10,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n        -1.1175871e-08,  0.0000000e+00,  0.0000000e+00,  3.7252903e-09,\n         0.0000000e+00,  0.0000000e+00,  5.9604645e-08,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  1.8626451e-09,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  1.1175871e-08,  1.4901161e-08,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.4901161e-08,\n         1.3969839e-09,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         7.4505806e-09,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00, -5.9604645e-08,  0.0000000e+00]],\n      dtype=float32)"},"metadata":{}}]}]}